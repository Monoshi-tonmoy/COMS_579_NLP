{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/monoshi/COMS_579_NLP\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!source /home/monoshi/venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monoshi/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_pdf(pdf_file):\n",
    "    try:\n",
    "        text_loader = PyPDFLoader(pdf_file)\n",
    "        text = text_loader.load_and_split()\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading PDF: {e}\")\n",
    "        return None\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=128, chunk_overlap=0)\n",
    "    split_docs = text_splitter.split_documents(text)\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [f for f in os.listdir(\"/home/monoshi/COMS_579_NLP/KB/\") if f.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monoshi/venv/lib/python3.10/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "WEAVIATE_URL = \"https://project-nl7mysdi.weaviate.network\"\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url=WEAVIATE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify embedding model (using huggingface sentence transformer)\n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "  model_name=embedding_model_name, \n",
    "  model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text=[]\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(\"/home/monoshi/COMS_579_NLP/KB/\", pdf_file)\n",
    "    text = upload_pdf(pdf_path)\n",
    "    all_text.extend(text)\n",
    "    chunked=chunk_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked=chunk_text(all_text)\n",
    "\n",
    "vector_db = Weaviate.from_documents(\n",
    "    chunked, embeddings, client=client, by_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='transformer,\\nLarge\\nLanguage\\nModels(LLMs)\\nhave\\nnot\\nonly\\nbecome\\nthe\\nforefront\\nof\\ntext\\ngeneration\\ntasks\\nbut\\nhave\\nstarted\\nto', metadata={'page': 0, 'source': '/home/monoshi/COMS_579_NLP/KB/Introduction.pdf'}), Document(page_content='Title:\\nCapabilities\\nof\\nLarge\\nLanguage\\nModels\\nin\\nProgram\\nAnalysis\\nTasks\\nIntroduction:\\nIn\\nrecent\\nyears,\\nafter\\nthe\\nemergence\\nof\\nthe', metadata={'page': 0, 'source': '/home/monoshi/COMS_579_NLP/KB/Introduction.pdf'}), Document(page_content='large\\nlanguage\\nmodels\\nand\\nmodel\\npredictive\\ncontrol\\nfor\\nbuildings\\noptimal\\noperation,\\nDec.\\n2023.\\ndoi:10.21203/rs.3.rs-3735947/v1', metadata={'page': 0, 'source': '/home/monoshi/COMS_579_NLP/KB/Introduction.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    vector_db.similarity_search(\n",
    "        \"What is the full form of Large Language Models?\", k=3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jupyterlab-widgets~=3.0.10\n",
      "  Downloading jupyterlab_widgets-3.0.10-py3-none-any.whl (215 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/215.0 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: comm>=0.1.3 in /home/monoshi/venv/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/monoshi/venv/lib/python3.10/site-packages (from ipywidgets) (5.14.2)\n",
      "Collecting widgetsnbextension~=4.0.10\n",
      "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ipython>=6.1.0 in /home/monoshi/venv/lib/python3.10/site-packages (from ipywidgets) (8.23.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/monoshi/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/monoshi/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /home/monoshi/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: matplotlib-inline in /home/monoshi/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/monoshi/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: decorator in /home/monoshi/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: typing-extensions in /home/monoshi/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/monoshi/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/monoshi/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/monoshi/venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/monoshi/venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/monoshi/venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/monoshi/venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: pure-eval in /home/monoshi/venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/monoshi/venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/monoshi/venv/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.2 jupyterlab-widgets-3.0.10 widgetsnbextension-4.0.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc083ed175a64255b3da57db32762d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install ipywidgets\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<00:00, 4.43MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 11.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 15.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.41MB/s]\n",
      "config.json: 100%|██████████| 587/587 [00:00<00:00, 2.00MB/s]\n",
      "model.safetensors.index.json: 100%|██████████| 33.4k/33.4k [00:00<00:00, 30.8MB/s]\n",
      "model-00001-of-00003.safetensors: 100%|██████████| 9.95G/9.95G [01:33<00:00, 106MB/s] \n",
      "model-00002-of-00003.safetensors: 100%|██████████| 9.90G/9.90G [01:32<00:00, 107MB/s]\n",
      "model-00003-of-00003.safetensors: 100%|██████████| 6.18G/6.18G [00:56<00:00, 109MB/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [04:03<00:00, 81.25s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.56s/it]\n",
      "generation_config.json: 100%|██████████| 188/188 [00:00<00:00, 644kB/s]\n"
     ]
    }
   ],
   "source": [
    "# specify model huggingface mode name\n",
    "model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "# function for loading 4-bit quantized model\n",
    "def load_quantized_model(model_name: str):\n",
    "    \"\"\"\n",
    "    :param model_name: Name or path of the model to be loaded.\n",
    "    :return: Loaded quantized model.\n",
    "    \"\"\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        token=os.getenv(\"HF_Token\"),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# function for initializing tokenizer\n",
    "def initialize_tokenizer(model_name: str):\n",
    "    \"\"\"\n",
    "    Initialize the tokenizer with the specified model_name.\n",
    "\n",
    "    :param model_name: Name or path of the model for tokenizer initialization.\n",
    "    :return: Initialized tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, return_token_type_ids=True)\n",
    "    tokenizer.bos_token_id = 1  # Set beginning of sentence token id\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "# initialize tokenizer\n",
    "tokenizer = initialize_tokenizer(model_name)\n",
    "# load model\n",
    "model = load_quantized_model(model_name)\n",
    "# specify stop token ids\n",
    "stop_token_ids = [0]\n",
    "\n",
    "\n",
    "# build huggingface pipeline for using zephyr-7b-alpha\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    use_cache=True,\n",
    "    device_map=\"auto\",\n",
    "    max_length=2048,\n",
    "    do_sample=True,\n",
    "    top_k=5,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vector_db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monoshi/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Mr .\n",
      "Whisk ers\n",
      "was\n",
      "no\n",
      "or dinar y\n",
      "cat.\n",
      "He\n",
      "had\n",
      "a\n",
      "knack\n",
      "for\n",
      "getting\n",
      "himself\n",
      "int o\n",
      "all\n",
      "sor ts\n",
      "of\n",
      "adv entur es,\n",
      "much\n",
      "t o\n",
      "the\n",
      "chagrin\n",
      "\n",
      "of\n",
      "his\n",
      "owner ,\n",
      "Mrs.\n",
      "Smith.\n",
      "One\n",
      "sunny\n",
      "morning,\n",
      "as\n",
      "Mrs.\n",
      "Smith\n",
      "was\n",
      "busy\n",
      "tending\n",
      "t o\n",
      "her\n",
      "gar den,\n",
      "Mr .\n",
      "Whisk ers\n",
      "spotted\n",
      "a\n",
      "butterﬂy\n",
      "\n",
      "Once\n",
      "upon\n",
      "a\n",
      "time\n",
      "in\n",
      "a\n",
      "small\n",
      "village\n",
      "nestled\n",
      "between\n",
      "r olling\n",
      "hills,\n",
      "ther e\n",
      "liv ed\n",
      "a\n",
      "curious\n",
      "little\n",
      "cat\n",
      "named\n",
      "Mr .\n",
      "Whisk ers.\n",
      "\n",
      "holding\n",
      "a\n",
      "nut\n",
      "in\n",
      "its\n",
      "tiny\n",
      "paws.\n",
      "With\n",
      "a\n",
      "twinkle\n",
      "in\n",
      "its\n",
      "e y e,\n",
      "the\n",
      "squirr el\n",
      "off er ed\n",
      "the\n",
      "nut\n",
      "t o\n",
      "Mr .\n",
      "Whisk ers,\n",
      "who\n",
      "eagerly\n",
      "\n",
      "Question: How is Mr. Whisker?\n",
      "Helpful Answer: Mr. Whiskers is a curious little cat who loves to get into all sorts of adventures, much to the chagrin of his owner, Mrs. Smith. One sunny morning, as Mrs. Smith was busy tending to her garden, Mr. Whiskers spotted a butterfly.\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain.run(\n",
    "    \"How is Mr. Whisker?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How is Mr. Whisker?\n",
      "Helpful Answer: Mr. Whiskers is a curious little cat who loves to get into all sorts of adventures, much to the chagrin of his owner, Mrs. Smith. One sunny morning, as Mrs. Smith was busy tending to her garden, Mr. Whiskers spotted a butterfly.\n"
     ]
    }
   ],
   "source": [
    "# Split the response string by newline characters (\"\\n\")\n",
    "response_lines = response.split(\"\\n\")\n",
    "\n",
    "for i in range(len(response_lines)):\n",
    "    if response_lines[i].startswith('Question:'):\n",
    "        print(f\"{response_lines[i]}\")\n",
    "    elif response_lines[i].startswith('Helpful Answer:'):\n",
    "        print(f\"{response_lines[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Mr. Whisker?\n",
      "\n",
      "What is the cost of Tech giants?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"Query/Questions.txt\") as file:\n",
    "    lines=file.readlines()\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    print(lines[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
